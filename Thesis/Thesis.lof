\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Example historical aerial images\relax }}{2}{figure.caption.7}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces The model that we use to produce an affine transformation matrix, $\theta $, from two source images, $I_A$ and $I_B$. \cite {Rocco17}\relax }}{4}{figure.caption.10}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces A listing of the layers involved in the VGG-16 Convolutional Neural Network Model.\cite {simonyan2014very}\relax }}{4}{figure.caption.12}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Single layer network with one output and two inputs\cite {krose1993introduction}\relax }}{7}{figure.caption.18}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces The Rectified Linear Unit (ReLU) activation function\cite {nair2010proceedings}\relax }}{8}{figure.caption.19}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Example image with horizontal and vertical features shown.\cite {wu2017introduction}\relax }}{9}{figure.caption.21}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces The architecture of a CNN\cite {cnn}\relax }}{9}{figure.caption.22}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces A pre-trained model (above) has the last layers (green) removed and replaced so that the model may be reused for a new task (below). This process is called Transfer Learning.\citep {TL}\relax }}{10}{figure.caption.24}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Example of the SIFT algorithm finding interest points (features) in an image.\cite {sift_cv}\relax }}{11}{figure.caption.26}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces Example of the RANSAC algorithm finding correspondences between two images.\cite {ransac_ski}\relax }}{11}{figure.caption.27}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces Aerial image alignment and stitching from three aerial photographs containing overlapping data.\relax }}{13}{figure.caption.29}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces 46.2382° N, 63.1311° W, Charlottetown\relax }}{14}{figure.caption.32}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Applying an affine transformation to the image at left, yields the transformed image at right. Notice the black bar on the left side of the transformed image.\relax }}{15}{figure.caption.34}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces training image pair\relax }}{16}{figure.caption.36}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces VGG16 Architecture\cite {vgg_arch}\relax }}{17}{figure.caption.39}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces CNN Model showing 3 steps in our process to arrive at $\theta $, which represents the transformation that can be applied to image $I_A$ to best align it with image $I_B$ (the inputs to the model).\cite {Rocco17}\relax }}{19}{figure.caption.42}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Two example images with red dots showing common points between the images. These images may be used as input to our model.\relax }}{20}{figure.caption.44}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces The full model use by Rocco et, al.\cite {cnngeometric_pytorch}. Their full model was intended to warp and twist portions of an image for best alignment. We do not use the full model, as we assume aerial photographs to be roughly taken from the same relative position to the earth. In other words we assume the plane flies horizontal with a camera pointing down.\relax }}{21}{figure.caption.47}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces In the demonstration by Rocco et al\cite {Rocco17}, they show the alignment of two images containing similar ducks\relax }}{21}{figure.caption.48}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces In the demonstration by Rocco et al\cite {Rocco18}, they show the finding the overlap between images\relax }}{21}{figure.caption.49}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces In the demonstration by Rocco et al\cite {Rocco18}, they show the bounding box that single out the key feature between images\relax }}{22}{figure.caption.50}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces Model demo \cite {cnngeometric_pytorch}\relax }}{22}{figure.caption.51}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces The source (src) and target (tgt) images at left are sent into the machine learning model to produce the affine transformation that may be applied to the source and produce the target. This is shown in the image under heading ml aff. Finally the actual ground truth affine transformation applied to the source image is shown. The ground truth affine transformation represents a shift of 20 pixels to the right and 20 pixels up.\relax }}{26}{figure.caption.59}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Source and Target Images sent into our model produce the ml aff image. Shown last is the actual ground truth solution. The affine transformation for this image pair is a 40 pixel shift right and 40 pixel shift down. Visually our model produces a pleasing result.\relax }}{27}{figure.caption.60}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Source and Target Images sent into our model produce the ml aff image. Shown last is the actual ground truth solution. The affine transformation for this image pair is a 60 pixel shift right and 60 pixel shift down.\relax }}{27}{figure.caption.61}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Applying SIFT and RANSAC to the source(src) and target(tgt) images produce the SIFT and RANSAC image are shown. While the ground truth is shown at far right. The ground truth normalized affine transformation matrix is [[1, 0, -0.1639 ] [0, 1, 0.1639] [0, 0, 1]] while with SIFT and RANSAC with normalization we get an affine transformation matrix of [[0.9999, 0.0, -0.1666] [0.0, 0.999, 0.1667] [0, 0, 1]]\relax }}{28}{figure.caption.63}%
\contentsline {figure}{\numberline {5.5}{\ignorespaces Applying SIFT and RANSAC to the source(src) and target(tgt) images produce the SIFT and RANSAC image are shown. While the ground truth is shown at far right. The ground truth normalized affine transformation matrix is [[1, 0, -0.3279 ] [0, 1, -0.3279] [0, 0, 1]] while with SIFT and RANSAC with normalization we get an affine transformation matrix of [[0.9999, 0.0, -0.3333] [0.0, 0.999, -0.3333] [0, 0, 1]]\relax }}{28}{figure.caption.64}%
\contentsline {figure}{\numberline {5.6}{\ignorespaces Applying SIFT and RANSAC to the source(src) and target(tgt) images produce the SIFT and RANSAC image are shown. While the ground truth is shown at far right. The ground truth normalized affine transformation matrix is [[1, 0, -0.4918 ] [0, 1, -0.4918] [0, 0, 1]] while with SIFT and RANSAC with normalization we get an affine transformation matrix of [[0.9999, 0.0, -0.4999] [0.0, 0.999, -0.4999] [0, 0, 1]]\relax }}{28}{figure.caption.65}%
\contentsline {figure}{\numberline {5.7}{\ignorespaces We pick 3 points from the source image(src) and 3 point from the target image(tgt) and generated the affine transformation from the points we picked(Mouse click) and compare it with ground truth affine transformation(true aff).\relax }}{29}{figure.caption.67}%
\contentsline {figure}{\numberline {5.8}{\ignorespaces We pick 3 points from the source image(src) and 3 point from the target image(tgt) and generated the affine transformation from the points we picked(Mouse click) and compare it with ground truth affine transformation(true aff).\relax }}{29}{figure.caption.68}%
\contentsline {figure}{\numberline {5.9}{\ignorespaces We pick 3 points from the source image(src) and 3 point from the target image(tgt) and generated the affine transformation from the points we picked(Mouse click) and compare it with ground truth affine transformation(true aff).\relax }}{29}{figure.caption.69}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces The comparison of training data between Rooco,I\citep {Rocco17} and us. The training data from Rocco\citep {Rocco17}(left side) has some key features in the center of the images, but it is not too obvious what is the key features in our training images(right size).\relax }}{32}{figure.caption.74}%
\contentsline {figure}{\numberline {6.2}{\ignorespaces This shows a image pair, the target image are 60 pixels translate right and 60 pixels translate down. We circle out the common point in the images, we can see that it is very hard to find strong features between this image pairs even the translation is not too big.\relax }}{32}{figure.caption.75}%
